{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running DeepSeek R1 Distilled LLaMA 70B\n",
    "\n",
    "This notebook provides a step-by-step guide to setting up and running the DeepSeek R1 Distilled LLaMA 70B model.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "### Hardware Requirements\n",
    "- GPU: NVIDIA GPU with minimum 40GB VRAM (A100, RTX 3090, or RTX 4090)\n",
    "- Optimal Setup: 2x NVIDIA A100 (40GB) or 4x NVIDIA RTX 3090 (24GB)\n",
    "- System RAM: 128GB recommended\n",
    "\n",
    "### Software Requirements\n",
    "- Python 3.12\n",
    "- CUDA 12.2\n",
    "- PyTorch 2.2.x\n",
    "- Transformers library\n",
    "- Accelerate library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA 12.2 support\n",
    "!pip install torch torchvision torchaudio\n",
    "\n",
    "# Install required libraries\n",
    "!pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU(s) available: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Loading and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def load_model(model_name=\"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\"):\n",
    "    \"\"\"\n",
    "    Load the DeepSeek model and tokenizer\n",
    "    \"\"\"\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Load model with optimizations\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,  # Use FP16 for efficiency\n",
    "        device_map=\"auto\",          # Automatically handle multi-GPU setup\n",
    "        trust_remote_code=True      # Required for some model configurations\n",
    "    )\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = load_model()\n",
    "print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    prompt,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    max_length=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    num_return_sequences=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate text using the DeepSeek model\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): Input text prompt\n",
    "        max_length (int): Maximum length of generated text\n",
    "        temperature (float): Controls randomness (higher = more random)\n",
    "        top_p (float): Nucleus sampling parameter\n",
    "        num_return_sequences (int): Number of sequences to generate\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate text\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        top_p=top_p,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode and return generated text\n",
    "    generated_texts = [\n",
    "        tokenizer.decode(output, skip_special_tokens=True)\n",
    "        for output in outputs\n",
    "    ]\n",
    "    \n",
    "    return generated_texts[0] if num_return_sequences == 1 else generated_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Example Usage\n",
    "\n",
    "### 5.1 Basic Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain the concept of quantum computing in simple terms.\"\n",
    "response = generate_text(prompt, model, tokenizer, max_length=150)\n",
    "print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Creative Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Write a short story about a robot discovering emotions.\n",
    "Theme: Self-discovery\n",
    "Length: Approximately 100 words\"\"\"\n",
    "\n",
    "story = generate_text(\n",
    "    prompt,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    max_length=200,\n",
    "    temperature=0.8\n",
    ")\n",
    "print(\"Generated Story:\\n\", story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Write a Python function to calculate the Fibonacci sequence up to n terms.\n",
    "Include docstring and type hints.\"\"\"\n",
    "\n",
    "code = generate_text(\n",
    "    prompt,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    max_length=250,\n",
    "    temperature=0.4  # Lower temperature for more focused code generation\n",
    ")\n",
    "print(\"Generated Code:\\n\", code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_summarize = \"\"\"\n",
    "The Industrial Revolution was a period of major industrialization that took place during \n",
    "the late 1700s and early 1800s. It began in Great Britain and spread to the rest of \n",
    "the world. This era saw the development of new technologies, such as the steam engine, \n",
    "which revolutionized manufacturing and transportation. The Industrial Revolution also led \n",
    "to significant social and economic changes, including urbanization and the rise of the \n",
    "working class.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"Summarize the following text in 2-3 sentences:\\n{text_to_summarize}\"\n",
    "\n",
    "summary = generate_text(\n",
    "    prompt,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    max_length=100,\n",
    "    temperature=0.3  # Lower temperature for more focused summarization\n",
    ")\n",
    "print(\"Summary:\\n\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Memory Management and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clear GPU memory if needed\n",
    "def clear_gpu_memory():\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            print(f\"GPU {i} memory allocated: {torch.cuda.memory_allocated(i) / 1e9:.2f} GB\")\n",
    "\n",
    "# Clear memory after processing\n",
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes and Best Practices\n",
    "\n",
    "1. **Memory Management**:\n",
    "   - Monitor GPU memory usage during generation\n",
    "   - Use `clear_gpu_memory()` function when needed\n",
    "   - Consider batch processing for multiple requests\n",
    "\n",
    "2. **Performance Optimization**:\n",
    "   - Use FP16 (half-precision) for efficient inference\n",
    "   - Adjust `max_length` based on your needs\n",
    "   - Use appropriate `temperature` values for different tasks\n",
    "\n",
    "3. **Error Handling**:\n",
    "   - Implement proper error handling in production\n",
    "   - Monitor for out-of-memory conditions\n",
    "   - Handle token length limitations\n",
    "\n",
    "4. **Multi-GPU Setup**:\n",
    "   - The model automatically handles multi-GPU distribution\n",
    "   - Ensure proper CUDA setup for multi-GPU usage\n",
    "   - Monitor individual GPU usage\n",
    "\n",
    "Remember to handle the model and GPU resources properly in production environments. This notebook is intended for research and development purposes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
